{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95382e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b89af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e21be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"Hello World! This is Akash Anup. I'm learning about tokenization. Tokenization is the process of splitting text into smaller pieces, called tokens. Tokens can be words, sentences, or even characters. In this example, we will focus on sentence tokenization. Sentence tokenization is the process of splitting a paragraph into sentences. It is useful for text analysis and natural language processing tasks. Let's see how to perform sentence tokenization using NLTK in Python.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795d5130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the corpus into sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edb8ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the corpus into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ba9006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the corpus into characters\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "regexp_tokenize(corpus, pattern=r'\\w+|\\s+|[^\\w\\s]', gaps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04265657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the corpus into Treebank sentences\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "treebank_tokenizer.tokenize(corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
